{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8767723,"sourceType":"datasetVersion","datasetId":5268544}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndata = pd.read_csv('/kaggle/input/weather-type-classification/weather_classification_data.csv')\nprint(data.head())\nprint(data.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:47:31.132725Z","iopub.execute_input":"2024-09-04T19:47:31.133527Z","iopub.status.idle":"2024-09-04T19:47:31.172131Z","shell.execute_reply.started":"2024-09-04T19:47:31.133482Z","shell.execute_reply":"2024-09-04T19:47:31.171150Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"   Temperature  Humidity  Wind Speed  Precipitation (%)    Cloud Cover  \\\n0         14.0        73         9.5               82.0  partly cloudy   \n1         39.0        96         8.5               71.0  partly cloudy   \n2         30.0        64         7.0               16.0          clear   \n3         38.0        83         1.5               82.0          clear   \n4         27.0        74        17.0               66.0       overcast   \n\n   Atmospheric Pressure  UV Index  Season  Visibility (km)  Location  \\\n0               1010.82         2  Winter              3.5    inland   \n1               1011.43         7  Spring             10.0    inland   \n2               1018.72         5  Spring              5.5  mountain   \n3               1026.25         7  Spring              1.0   coastal   \n4                990.67         1  Winter              2.5  mountain   \n\n  Weather Type  \n0        Rainy  \n1       Cloudy  \n2        Sunny  \n3        Sunny  \n4        Rainy  \nIndex(['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)',\n       'Cloud Cover', 'Atmospheric Pressure', 'UV Index', 'Season',\n       'Visibility (km)', 'Location', 'Weather Type'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"x = data[['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)', 'Cloud Cover',\n          'Atmospheric Pressure', 'UV Index', 'Season', 'Visibility (km)', 'Location']]\ny = data['Weather Type']\n#considering y as the target variable as always","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:47:37.143884Z","iopub.execute_input":"2024-09-04T19:47:37.144866Z","iopub.status.idle":"2024-09-04T19:47:37.153461Z","shell.execute_reply.started":"2024-09-04T19:47:37.144807Z","shell.execute_reply":"2024-09-04T19:47:37.152302Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Encoding categorical data\nx.columns = x.columns.str.strip()\nlabel_encoders = {}\ncategorical_columns = ['Cloud Cover', 'Location', 'Season']\n\nfor column in categorical_columns:\n    le = LabelEncoder()\n    x.loc[:, column] = le.fit_transform(x.loc[:, column])\n    label_encoders[column] = le\nle_weather_type = LabelEncoder()\ny = le_weather_type.fit_transform(y)\n# Normalize numerical data\nnumerical_features = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)',\n                       'Atmospheric Pressure', 'UV Index', 'Visibility (km)']\nscaler = StandardScaler()\nx.loc[:,numerical_features] = x.loc[:,numerical_features].astype('float64')\nx.loc[:, numerical_features] = scaler.fit_transform(x.loc[:, numerical_features])","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:47:41.449264Z","iopub.execute_input":"2024-09-04T19:47:41.449647Z","iopub.status.idle":"2024-09-04T19:47:41.487446Z","shell.execute_reply.started":"2024-09-04T19:47:41.449611Z","shell.execute_reply":"2024-09-04T19:47:41.486440Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/1491302430.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.21240351  1.35138482 -0.23328483 ...  0.41048722  0.36096629\n -1.52082893]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  x.loc[:, numerical_features] = scaler.fit_transform(x.loc[:, numerical_features])\n/tmp/ipykernel_37/1491302430.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.52010407  0.77642368  0.25781258 ... -0.26079852 -1.03871517\n  0.25781258]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  x.loc[:, numerical_features] = scaler.fit_transform(x.loc[:, numerical_features])\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:47:54.799893Z","iopub.execute_input":"2024-09-04T19:47:54.800822Z","iopub.status.idle":"2024-09-04T19:47:54.810747Z","shell.execute_reply.started":"2024-09-04T19:47:54.800781Z","shell.execute_reply":"2024-09-04T19:47:54.809811Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(10560, 10)\n(2640, 10)\n(10560,)\n(2640,)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Model definition\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=x_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(len(le_weather_type.classes_), activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:48:08.171637Z","iopub.execute_input":"2024-09-04T19:48:08.172026Z","iopub.status.idle":"2024-09-04T19:48:08.212200Z","shell.execute_reply.started":"2024-09-04T19:48:08.171989Z","shell.execute_reply":"2024-09-04T19:48:08.211205Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**TRAINING THE MODEL AND THE ACCURACY BELOW REFERS TO TRAINING'S ACCURACY**","metadata":{}},{"cell_type":"code","source":"#Model training\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, epochs=20, batch_size=10, validation_split=0.1)\ntrain_loss = history.history['loss']\ntrain_accuracy = history.history['accuracy']\nval_loss = history.history['val_loss']\nval_accuracy = history.history['val_accuracy']\nprint(f\"Final Training Accuracy: {train_accuracy[-1]:.4f}\")\nprint(f\"Final Validation Accuracy: {val_accuracy[-1]:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:48:15.539268Z","iopub.execute_input":"2024-09-04T19:48:15.539666Z","iopub.status.idle":"2024-09-04T19:48:49.238115Z","shell.execute_reply.started":"2024-09-04T19:48:15.539628Z","shell.execute_reply":"2024-09-04T19:48:49.237197Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7903 - loss: 0.6447 - val_accuracy: 0.9148 - val_loss: 0.2964\nEpoch 2/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9022 - loss: 0.2859 - val_accuracy: 0.9138 - val_loss: 0.2550\nEpoch 3/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8991 - loss: 0.2716 - val_accuracy: 0.9167 - val_loss: 0.2355\nEpoch 4/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9074 - loss: 0.2358 - val_accuracy: 0.9205 - val_loss: 0.2244\nEpoch 5/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9064 - loss: 0.2325 - val_accuracy: 0.9223 - val_loss: 0.2175\nEpoch 6/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9101 - loss: 0.2178 - val_accuracy: 0.9223 - val_loss: 0.1977\nEpoch 7/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9131 - loss: 0.2078 - val_accuracy: 0.9186 - val_loss: 0.2101\nEpoch 8/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9205 - loss: 0.2036 - val_accuracy: 0.9214 - val_loss: 0.1998\nEpoch 9/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9197 - loss: 0.1920 - val_accuracy: 0.9223 - val_loss: 0.1894\nEpoch 10/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9175 - loss: 0.1893 - val_accuracy: 0.9214 - val_loss: 0.1971\nEpoch 11/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9208 - loss: 0.1890 - val_accuracy: 0.9214 - val_loss: 0.1923\nEpoch 12/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9226 - loss: 0.1819 - val_accuracy: 0.9176 - val_loss: 0.1900\nEpoch 13/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9262 - loss: 0.1731 - val_accuracy: 0.9195 - val_loss: 0.1885\nEpoch 14/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9258 - loss: 0.1778 - val_accuracy: 0.9252 - val_loss: 0.1806\nEpoch 15/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9306 - loss: 0.1613 - val_accuracy: 0.9167 - val_loss: 0.2016\nEpoch 16/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9315 - loss: 0.1664 - val_accuracy: 0.9205 - val_loss: 0.1913\nEpoch 17/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9309 - loss: 0.1553 - val_accuracy: 0.9186 - val_loss: 0.1987\nEpoch 18/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9308 - loss: 0.1631 - val_accuracy: 0.9223 - val_loss: 0.1886\nEpoch 19/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.1585 - val_accuracy: 0.9271 - val_loss: 0.1842\nEpoch 20/20\n\u001b[1m951/951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9315 - loss: 0.1588 - val_accuracy: 0.9167 - val_loss: 0.2137\nFinal Training Accuracy: 0.9297\nFinal Validation Accuracy: 0.9167\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**THE FOLLOWING OUTPUTS REPRESENT THE TESTING PART**","metadata":{}},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:48:58.164652Z","iopub.execute_input":"2024-09-04T19:48:58.165513Z","iopub.status.idle":"2024-09-04T19:48:58.646044Z","shell.execute_reply.started":"2024-09-04T19:48:58.165468Z","shell.execute_reply":"2024-09-04T19:48:58.645006Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9011 - loss: 0.2408\nTest Loss: 0.2377\nTest Accuracy: 0.9011\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = model.predict(x_test)\npredictions = predictions.argmax(axis=-1)\nconf_matrix = confusion_matrix(y_test, predictions)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nclass_report = classification_report(y_test, predictions, target_names=le_weather_type.classes_)\nprint(\"Classification Report:\")\nprint(class_report)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:49:06.665651Z","iopub.execute_input":"2024-09-04T19:49:06.666405Z","iopub.status.idle":"2024-09-04T19:49:07.059488Z","shell.execute_reply.started":"2024-09-04T19:49:06.666365Z","shell.execute_reply":"2024-09-04T19:49:07.058532Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\nConfusion Matrix:\n[[564  55   8  24]\n [ 39 588   4  16]\n [ 11  17 655  18]\n [ 37  25   7 572]]\nClassification Report:\n              precision    recall  f1-score   support\n\n      Cloudy       0.87      0.87      0.87       651\n       Rainy       0.86      0.91      0.88       647\n       Snowy       0.97      0.93      0.95       701\n       Sunny       0.91      0.89      0.90       641\n\n    accuracy                           0.90      2640\n   macro avg       0.90      0.90      0.90      2640\nweighted avg       0.90      0.90      0.90      2640\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**FINAL INFERENCES**","metadata":{}},{"cell_type":"code","source":"print(\"Training Accuracy: \",train_accuracy[-1])\nprint(\"Testing Accuracy: \",test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:50:36.034002Z","iopub.execute_input":"2024-09-04T19:50:36.034749Z","iopub.status.idle":"2024-09-04T19:50:36.040038Z","shell.execute_reply.started":"2024-09-04T19:50:36.034706Z","shell.execute_reply":"2024-09-04T19:50:36.039008Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Training Accuracy:  0.929713785648346\nTesting Accuracy:  0.9011363387107849\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Thus, the case of overfitting is not encountered and testing and training both have similar accruacies. Hence the neural netowrk works fine, probably :)**","metadata":{}}]}